import requests  # 添加请求
import json
import streamlit as st


from groq import Groq
from langchain.chains import ConversationChain, LLMChain
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq


# 使用请求从网络获取数据
url = "https://google.serper.dev/search"

groq_api_key = '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'

# Add customization options to the sidebar
st.sidebar.title('Customization')
system_prompt = st.sidebar.text_input("System prompt:")
model = st.sidebar.selectbox(
    'Choose a model',
    ['llama-3.1-70b-versatile', 'llama-3.1-8b-instant',
        'llama3-70b-8192', 'llama3-8b-8192']
)

groq_chat = ChatGroq(groq_api_key=groq_api_key, model_name=model)


query = st.text_input('Please input the query:')

response = None

if query:
    payload = json.dumps({
        "q": query
    })

    headers = {
        'X-API-KEY': '2203d27aa32a1d92275134fb632bf009714b2476',
        'Content-Type': 'application/json'
    }

    response = requests.request("POST", url, headers=headers, data=payload)

    # st.write(response.text)

if response.text and response:
    # Construct a chat prompt template using various components
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content=system_prompt
            ),  # This is the persistent system prompt that is always included at the start of the chat.

            MessagesPlaceholder(
                variable_name="chat_history"
            ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

            HumanMessagePromptTemplate.from_template(
                "{human_input}"
            ),  # This template is where the user's current input will be injected into the prompt.
        ]
    )

    # Create a conversation chain using the LangChain LLM (Language Learning Model)
    conversation = LLMChain(
        llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
        prompt=prompt,  # The constructed prompt template.
        # Enables verbose output, which can be useful for debugging.
        verbose=True,
    )

    # The chatbot's answer is generated by sending the full prompt to the Groq API.
    response = conversation.predict(human_input=response.text)
    st.write("Chatbot:", response)
