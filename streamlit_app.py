import streamlit as st
from langchain.chains import LLMChain
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.messages import SystemMessage
from langchain_groq import ChatGroq
import requests
import json


# Serper API Key
serper_api_key = "2203d27aa32a1d92275134fb632bf009714b2476"

# Groq API Key
groq_api_key = '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'

def main():
    # Add customization options to the sidebar
    st.sidebar.title('Customization')
    system_prompt = st.sidebar.text_input("System prompt:")
    model = st.sidebar.selectbox(
        'Choose a model',
        ['llama-3.1-70b-versatile', 'llama-3.1-8b-instant',
            'llama3-70b-8192', 'llama3-8b-8192']
    )

    groq_chat = ChatGroq(groq_api_key=groq_api_key, model_name=model)

    query = st.text_input('Please input the query:')

    if query:
        # Perform search using Serper
        url = "https://google.serper.dev/search"
        payload = json.dumps({
            "q": query
        })

        headers = {
            'X-API-KEY': serper_api_key,
            'Content-Type': 'application/json'
        }

        response = requests.request("POST", url, headers=headers, data=payload)

        # Combine search results into a single string
        search_results = response.json()
        search_results_text = " ".join([result["title"] + ". " + result["snippet"] for result in search_results])

        # Construct a chat prompt template using various components
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=system_prompt
                ),  # This is the persistent system prompt that is always included at the start of the chat.

                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # This template is where the user's current input will be injected into the prompt.
            ]
        )

        # Create a conversation chain using the LangChain LLM (Language Learning Model)
        conversation = LLMChain(
            llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
            prompt=prompt,  # The constructed prompt template.
            # Enables verbose output, which can be useful for debugging.
            verbose=True,
        )

        # The chatbot's answer is generated by sending the full prompt to the Groq API.
        response = conversation.predict(human_input="Summarize the following search results: " + search_results_text)
        st.write("Chatbot:", response)

if __name__ == "__main__":
    main()